{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "spoken-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepctr.models import DeepFM, xDeepFM\n",
    "import deepctr.feature_column\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat\n",
    "import ast\n",
    "from pickle import dump\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "incorporate-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('scores.csv')\n",
    "scores['scores'] = scores['scores'].apply(lambda x: ast.literal_eval(x))\n",
    "df2 = pd.json_normalize(scores['scores'])\n",
    "df2['feed_id'] = scores['feed_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "portuguese-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"./movielens_sample.txt\")\n",
    "df = pd.read_csv('shrunk.csv')\n",
    "# true or false is_premium for user\n",
    "is_premium_user = pd.read_csv('is_premium.csv')\n",
    "\n",
    "is_active = pd.read_csv('new-input.csv')\n",
    "premiums = pd.read_csv('premium_subs.csv')\n",
    "actives = pd.read_csv('active-subs.csv')\n",
    "num = pd.read_csv('num-subs.csv')\n",
    "avg_stories = pd.read_csv('avg_strs_pr_mnth.csv')\n",
    "data = df.merge(premiums[['premium_subs', 'feed_id']], how = 'left',\n",
    "                    left_on = 'feed_id', right_on = 'feed_id')\n",
    "\n",
    "data = data.merge(actives[['active_subs', 'feed_id']], how = 'left',\n",
    "                    left_on = 'feed_id', right_on = 'feed_id')\n",
    "\n",
    "data = data.merge(num[['num_subs', 'feed_id']], how = 'left',\n",
    "                    left_on = 'feed_id', right_on = 'feed_id')\n",
    "\n",
    "data = data.merge(df2[['read_pct', 'feed_id', 'reader_count', 'reach_score', 'story_count', 'share_count']], how = 'left',\n",
    "                    left_on = 'feed_id', right_on = 'feed_id')\n",
    "\n",
    "data = data.merge(avg_stories[['average_stories_per_month', 'feed_id']], how = 'left',\n",
    "                    left_on = 'feed_id', right_on = 'feed_id')\n",
    "\n",
    "# active refers to feed, from Feed class\n",
    "\n",
    "data = data.merge(is_active[['active', 'feed_id', 'active_premium_subscribers']], how = 'left',\n",
    "                    left_on = 'feed_id', right_on = 'feed_id')\n",
    "\n",
    "data = data.merge(is_premium_user[['is_premium', 'user',]], how = 'left',\n",
    "                    left_on = 'user', right_on = 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "opened-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['active'] = data['active'].apply(lambda x: int(x == True))\n",
    "data['is_premium'] = data['is_premium'].apply(lambda x: int(x == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "federal-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"feed_id\", \"user\", \"active\", \"is_premium\"]\n",
    "\n",
    "dense_features = [\"premium_subs\", \"active_subs\", \"num_subs\", \"read_pct\", \"reader_count\", \"reach_score\", \"story_count\", \"share_count\", \"average_stories_per_month\", 'active_premium_subscribers']\n",
    "target = ['is_following_feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "earned-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "        dump(lbe, open(feat + '-' + 'lbe.pkl', 'wb'))\n",
    "        \n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "# shouldn't need to save a ranged model\n",
    "#dump(mms, open('minmax.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "excited-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write our vocab sizes to file\n",
    "file = open(\"vocab-sizes.txt\", \"w\")\n",
    "for feat in sparse_features:\n",
    "    file.write(str(data[feat].max()+1) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "smaller-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sparse features, we transform them into dense vectors by embedding techniques. For dense numerical features, \n",
    "# we concatenate them to the input tensors of fully connected layer.\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1,embedding_dim=16)\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "vulnerable-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "SparseFeat(name='active', vocabulary_size=2, embedding_dim=16, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7fd0ca3f34d0>, embedding_name='active', group_name='default_group', trainable=True)\n"
     ]
    }
   ],
   "source": [
    "print(type(fixlen_feature_columns))\n",
    "print(fixlen_feature_columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "described-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we add variable length features we need this\n",
    "# linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "# dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "floating-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = deepctr.feature_column.get_feature_names(linear_feature_columns + dnn_feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "every-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "train_model_input = {name:train[name] for name in feature_names}\n",
    "test_model_input = {name:test[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "signed-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "excellent-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    if epoch < 30:\n",
    "        return 0.001\n",
    "    elif epoch < 37:\n",
    "        return 0.0008\n",
    "    else:\n",
    "        return 0.0005\n",
    "lr_scheduler = LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "automotive-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "INFO:tensorflow:batch_all_reduce: 19 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 19 all-reduces with algorithm = nccl, num_packs = 1\n",
      "12938/12938 - 322s - loss: 0.3348 - binary_crossentropy: 0.3191 - val_loss: 0.3286 - val_binary_crossentropy: 0.3099\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.30988, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 2/35\n",
      "12938/12938 - 310s - loss: 0.3253 - binary_crossentropy: 0.3027 - val_loss: 0.3226 - val_binary_crossentropy: 0.2997\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.30988 to 0.29968, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 3/35\n",
      "12938/12938 - 309s - loss: 0.3186 - binary_crossentropy: 0.2924 - val_loss: 0.3224 - val_binary_crossentropy: 0.2967\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.29968 to 0.29668, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 4/35\n",
      "12938/12938 - 310s - loss: 0.3138 - binary_crossentropy: 0.2848 - val_loss: 0.3191 - val_binary_crossentropy: 0.2920\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.29668 to 0.29196, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 5/35\n",
      "12938/12938 - 309s - loss: 0.3103 - binary_crossentropy: 0.2798 - val_loss: 0.3164 - val_binary_crossentropy: 0.2880\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.29196 to 0.28801, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 6/35\n",
      "12938/12938 - 309s - loss: 0.3083 - binary_crossentropy: 0.2767 - val_loss: 0.3186 - val_binary_crossentropy: 0.2888\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy did not improve from 0.28801\n",
      "Epoch 7/35\n",
      "12938/12938 - 310s - loss: 0.3067 - binary_crossentropy: 0.2740 - val_loss: 0.3159 - val_binary_crossentropy: 0.2858\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.28801 to 0.28584, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 8/35\n",
      "12938/12938 - 310s - loss: 0.3050 - binary_crossentropy: 0.2720 - val_loss: 0.3152 - val_binary_crossentropy: 0.2847\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.28584 to 0.28475, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 9/35\n",
      "12938/12938 - 309s - loss: 0.3039 - binary_crossentropy: 0.2703 - val_loss: 0.3141 - val_binary_crossentropy: 0.2835\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.28475 to 0.28349, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 10/35\n",
      "12938/12938 - 310s - loss: 0.3032 - binary_crossentropy: 0.2692 - val_loss: 0.3143 - val_binary_crossentropy: 0.2829\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.28349 to 0.28294, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 11/35\n",
      "12938/12938 - 310s - loss: 0.3025 - binary_crossentropy: 0.2679 - val_loss: 0.3143 - val_binary_crossentropy: 0.2827\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.28294 to 0.28271, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 12/35\n",
      "12938/12938 - 310s - loss: 0.3014 - binary_crossentropy: 0.2666 - val_loss: 0.3135 - val_binary_crossentropy: 0.2818\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.28271 to 0.28178, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 13/35\n",
      "12938/12938 - 311s - loss: 0.3008 - binary_crossentropy: 0.2657 - val_loss: 0.3136 - val_binary_crossentropy: 0.2817\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.28178 to 0.28168, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 14/35\n",
      "12938/12938 - 310s - loss: 0.3000 - binary_crossentropy: 0.2646 - val_loss: 0.3129 - val_binary_crossentropy: 0.2808\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.28168 to 0.28077, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 15/35\n",
      "12938/12938 - 309s - loss: 0.2993 - binary_crossentropy: 0.2635 - val_loss: 0.3130 - val_binary_crossentropy: 0.2806\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.28077 to 0.28062, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 16/35\n",
      "12938/12938 - 310s - loss: 0.2989 - binary_crossentropy: 0.2626 - val_loss: 0.3133 - val_binary_crossentropy: 0.2804\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy improved from 0.28062 to 0.28037, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 17/35\n",
      "12938/12938 - 310s - loss: 0.2981 - binary_crossentropy: 0.2616 - val_loss: 0.3129 - val_binary_crossentropy: 0.2800\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy improved from 0.28037 to 0.27997, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 18/35\n",
      "12938/12938 - 309s - loss: 0.2979 - binary_crossentropy: 0.2611 - val_loss: 0.3130 - val_binary_crossentropy: 0.2797\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.27997 to 0.27968, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 19/35\n",
      "12938/12938 - 310s - loss: 0.2974 - binary_crossentropy: 0.2602 - val_loss: 0.3127 - val_binary_crossentropy: 0.2791\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.27968 to 0.27906, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 20/35\n",
      "12938/12938 - 310s - loss: 0.2971 - binary_crossentropy: 0.2598 - val_loss: 0.3133 - val_binary_crossentropy: 0.2796\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy did not improve from 0.27906\n",
      "Epoch 21/35\n",
      "12938/12938 - 309s - loss: 0.2967 - binary_crossentropy: 0.2591 - val_loss: 0.3138 - val_binary_crossentropy: 0.2795\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy did not improve from 0.27906\n",
      "Epoch 22/35\n",
      "12938/12938 - 309s - loss: 0.2962 - binary_crossentropy: 0.2584 - val_loss: 0.3145 - val_binary_crossentropy: 0.2803\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy did not improve from 0.27906\n",
      "Epoch 23/35\n",
      "12938/12938 - 310s - loss: 0.2959 - binary_crossentropy: 0.2579 - val_loss: 0.3134 - val_binary_crossentropy: 0.2788\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.27906 to 0.27877, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 24/35\n",
      "12938/12938 - 311s - loss: 0.2955 - binary_crossentropy: 0.2574 - val_loss: 0.3134 - val_binary_crossentropy: 0.2786\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.27877 to 0.27863, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 25/35\n",
      "12938/12938 - 309s - loss: 0.2953 - binary_crossentropy: 0.2571 - val_loss: 0.3170 - val_binary_crossentropy: 0.2823\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.27863\n",
      "Epoch 26/35\n",
      "12938/12938 - 310s - loss: 0.2950 - binary_crossentropy: 0.2568 - val_loss: 0.3138 - val_binary_crossentropy: 0.2791\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.27863\n",
      "Epoch 27/35\n",
      "12938/12938 - 310s - loss: 0.2944 - binary_crossentropy: 0.2562 - val_loss: 0.3139 - val_binary_crossentropy: 0.2791\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.27863\n",
      "Epoch 28/35\n",
      "12938/12938 - 309s - loss: 0.2943 - binary_crossentropy: 0.2560 - val_loss: 0.3136 - val_binary_crossentropy: 0.2787\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.27863\n",
      "Epoch 29/35\n",
      "12938/12938 - 310s - loss: 0.2939 - binary_crossentropy: 0.2556 - val_loss: 0.3128 - val_binary_crossentropy: 0.2777\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy improved from 0.27863 to 0.27772, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 30/35\n",
      "12938/12938 - 309s - loss: 0.2938 - binary_crossentropy: 0.2555 - val_loss: 0.3123 - val_binary_crossentropy: 0.2775\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy improved from 0.27772 to 0.27754, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 31/35\n",
      "12938/12938 - 310s - loss: 0.2812 - binary_crossentropy: 0.2475 - val_loss: 0.3079 - val_binary_crossentropy: 0.2766\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy improved from 0.27754 to 0.27660, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 32/35\n",
      "12938/12938 - 309s - loss: 0.2766 - binary_crossentropy: 0.2430 - val_loss: 0.3085 - val_binary_crossentropy: 0.2768\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.27660\n",
      "Epoch 33/35\n",
      "12938/12938 - 310s - loss: 0.2754 - binary_crossentropy: 0.2414 - val_loss: 0.3082 - val_binary_crossentropy: 0.2759\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy improved from 0.27660 to 0.27593, saving model to best_model_fm_emb16_4layers.keras\n",
      "Epoch 34/35\n",
      "12938/12938 - 309s - loss: 0.2746 - binary_crossentropy: 0.2404 - val_loss: 0.3104 - val_binary_crossentropy: 0.2778\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.27593\n",
      "Epoch 35/35\n",
      "12938/12938 - 309s - loss: 0.2741 - binary_crossentropy: 0.2397 - val_loss: 0.3091 - val_binary_crossentropy: 0.2762\n",
      "\n",
      "Epoch 00035: val_binary_crossentropy did not improve from 0.27593\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_hidden_units=(128,128,128,128))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    checkpointer = ModelCheckpoint(monitor='val_binary_crossentropy',filepath='best_model_fm_emb16_4layers.keras', verbose=1, save_best_only=True)\n",
    "    model.compile(optimizer, \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "    history = model.fit(train_model_input, train[target].values,\n",
    "                        batch_size=256, epochs=35, verbose=2, validation_split=0.2, callbacks = [checkpointer, lr_scheduler])\n",
    "    pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "prospective-balloon",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-43-e2c3fefbbfae>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-e2c3fefbbfae>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    plt.title('model validation loss')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['val_binary_crossentropy'])\n",
    "        plt.title('model validation loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train'], loc='upper left')\n",
    "        plt.show()\n",
    "        plt.savefig('model-loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('deepfm-8-embed-3-layers.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to impl split for test/train \n",
    "# need to add evaluation function\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "color-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_not_interacted_list(masked_df, columns, items):\n",
    "    \"\"\" create better not interacted list\n",
    "    masked_df: full dataset df with user mask already removed\n",
    "    columns: columns needed for our model prediction\n",
    "    \"\"\"\n",
    "\n",
    "    df = masked_df.drop(masked_df[masked_df.reader_count == 0.0].index)[columns]\n",
    "    not_interacted_items = set(df['feed_id'].unique()) - set(items)\n",
    "    return not_interacted_items\n",
    "    \n",
    "def evaluation(test, model, full_df, features):\n",
    "    df = test.drop(test[test.is_following_feed != 1].index)[features]\n",
    "\n",
    "    \n",
    "    hits = []\n",
    "    counter = 0\n",
    "    input_dict = {}\n",
    "    for index, test_row in df.iterrows():\n",
    "        # get rows from full df \n",
    "        user_id = test_row['user']\n",
    "        mask = full_df['user'] == user_id\n",
    "        # full_df[mask]\n",
    "\n",
    "        items = list(full_df[mask]['feed_id'])\n",
    "        \n",
    "        #selected_not_interacted_list(full_df[full_df['user'] != user_id], features)\n",
    "        \n",
    "        not_interacted_items = set(full_df['feed_id'].unique()) - set(items)\n",
    "        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "        \n",
    "        # might not be sparse enough, might add 15% back from reader_count != 0.0 list to add more variation\n",
    "        # not sure why I still need to subract items, I guess they should pass a mask with feeds\n",
    "        \n",
    "        if bool(len({*items} & {*selected_not_interacted})):\n",
    "            raise ValueError\n",
    "        input_df = pd.DataFrame(columns=features)\n",
    "        # need to grab the extra data needed for \n",
    "        for feed in selected_not_interacted:\n",
    "            rows = full_df.loc[full_df['feed_id'] == feed]\n",
    "            \n",
    "            first = rows.iloc[0]\n",
    "\n",
    "            input_df = input_df.append(first)\n",
    "\n",
    "        \n",
    "        # add our final correct one on the end\n",
    "        \n",
    "        input_df.loc[:, 'is_following_feed'] = 0\n",
    "        input_df.loc[:, 'user'] = user_id\n",
    "        test_row['is_following_feed'] = 1\n",
    "        input_df = input_df.append(test_row)\n",
    "        \n",
    "        \n",
    "        input_df = input_df.drop(['is_following_feed', 'Unnamed: 0'], axis=1)\n",
    "        # predict with correct input format\n",
    "        pred_ans = model.predict({name:input_df[name] for name in features})\n",
    "  \n",
    "\n",
    "        # convert predictions to a little bit better format\n",
    "        predictions = [i[0] for i in pred_ans]\n",
    "        \n",
    "        feeds = input_df['feed_id'].tolist()\n",
    "        \n",
    "        results = sorted(dict(zip(feeds, predictions)).items(),  key=lambda x: x[1], reverse=True)\n",
    "        counter = counter + 1\n",
    "        if counter % 100 == 0:\n",
    "            print('we are at step: ' + str(counter))\n",
    "            print(\"the hit ratio at this step is {:.2f}\".format(np.average(hits)))\n",
    "        \n",
    "        top10_items = [i[0] for i in results[0:10]]\n",
    "        if test_row['feed_id'] in top10_items:\n",
    "            hits.append(1)\n",
    "#         print('we hit for feed: ' + str(u))\n",
    "        else:\n",
    "            hits.append(0)\n",
    "#         print('we missed for feed: ' + str(u))\n",
    "\n",
    "    print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "automotive-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /opt/conda/lib/python3.7/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot) (2.4.7)\n",
      "Requirement already satisfied: pydotplus in /opt/conda/lib/python3.7/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from pydotplus) (2.4.7)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "graphviz is already the newest version (2.40.1-6).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pydot\n",
    "# ! pip install pydotplus\n",
    "# ! sudo apt-get install graphviz\n",
    "#! pip install install keras-visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funky-prairie",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1a9d640369a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_visualizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvisualizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#files.download('current-model.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdense_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras_visualizer/__init__.py\u001b[0m in \u001b[0;36mvisualizer\u001b[0;34m(model, filename, format, view)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDigraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from deepctr.layers import custom_objects\n",
    "from keras_visualizer import visualizer\n",
    "\n",
    "model = keras.models.load_model('fm_emb16_4layers_.63HR.keras', custom_objects)\n",
    "\n",
    "\n",
    "# visualizer(model, format='png', view=True)\n",
    "#files.download('current-model.png')\n",
    "evaluation(test, model, data, sparse_features + dense_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-munich",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
